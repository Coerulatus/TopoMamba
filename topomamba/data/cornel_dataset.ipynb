{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add manually root '/home/lev/projects/TopoBenchmarkX'\n",
    "root_path = \"/home/lev/projects/TopoBenchmarkX\"\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import os.path as osp\n",
    "from collections.abc import Callable\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.io import fs\n",
    "\n",
    "from topobenchmarkx.io.load.download_utils import download_file_from_drive\n",
    "\n",
    "\n",
    "class CornelDataset(InMemoryDataset):\n",
    "    r\"\"\" \"\"\"\n",
    "\n",
    "    URLS = {\n",
    "        # 'contact-high-school': 'https://drive.google.com/open?id=1VA2P62awVYgluOIh1W4NZQQgkQCBk-Eu',\n",
    "        \"US-county-demos\": \"https://drive.google.com/file/d/1FNF_LbByhYNICPNdT6tMaJI9FxuSvvLK/view?usp=sharing\",\n",
    "    }\n",
    "\n",
    "    FILE_FORMAT = {\n",
    "        # 'contact-high-school': 'tar.gz',\n",
    "        \"US-county-demos\": \"zip\",\n",
    "    }\n",
    "\n",
    "    RAW_FILE_NAMES = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        parameters: dict = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        pre_filter: Optional[Callable] = None,\n",
    "        force_reload: bool = True,\n",
    "        use_node_attr: bool = False,\n",
    "        use_edge_attr: bool = False,\n",
    "    ) -> None:\n",
    "        self.name = name.replace(\"_\", \"-\")\n",
    "\n",
    "        super().__init__(\n",
    "            root, transform, pre_transform, pre_filter, force_reload=force_reload\n",
    "        )\n",
    "\n",
    "        # Step 3:Load the processed data\n",
    "        # After the data has been downloaded from source\n",
    "        # Then preprocessed to obtain x,y and saved into processed folder\n",
    "        # We can now load the processed data from processed folder\n",
    "\n",
    "        # Load the processed data\n",
    "        data, _, _ = fs.torch_load(self.processed_paths[0])\n",
    "\n",
    "        # Map the loaded data into\n",
    "        data = Data.from_dict(data)\n",
    "\n",
    "        # Step 5: Create the splits and upload desired fold\n",
    "\n",
    "        # split_idx = random_splitting(data.y, parameters=self.parameters)\n",
    "\n",
    "        # Assign data object to self.data, to make it be prodessed by Dataset class\n",
    "        self.data = data\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"processed\")\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list[str]:\n",
    "        names = [\"\", \"_2012\"]\n",
    "        return [f\"{self.name}_{name}.txt\" for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return \"data.pt\"\n",
    "\n",
    "    def download(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the dataset from the specified URL and saves it to the raw directory.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset URL is not found.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Download data from the source\n",
    "        self.url = self.URLS[self.name]\n",
    "        self.file_format = self.FILE_FORMAT[self.name]\n",
    "\n",
    "        download_file_from_drive(\n",
    "            file_link=self.url,\n",
    "            path_to_save=self.raw_dir,\n",
    "            dataset_name=self.name,\n",
    "            file_format=self.file_format,\n",
    "        )\n",
    "\n",
    "        # Extract the downloaded file if it is compressed\n",
    "        fs.cp(\n",
    "            f\"{self.raw_dir}/{self.name}.{self.file_format}\", self.raw_dir, extract=True\n",
    "        )\n",
    "\n",
    "        # Move the etracted files to the datasets/domain/dataset_name/raw/ directory\n",
    "        for filename in fs.ls(osp.join(self.raw_dir, self.name)):\n",
    "            fs.mv(filename, osp.join(self.raw_dir, osp.basename(filename)))\n",
    "        fs.rm(osp.join(self.raw_dir, self.name))\n",
    "\n",
    "        # Delete also f'{self.raw_dir}/{self.name}.{self.file_format}'\n",
    "        fs.rm(f\"{self.raw_dir}/{self.name}.{self.file_format}\")\n",
    "\n",
    "    def process(self) -> None:\n",
    "        \"\"\"\n",
    "        Process the data for the dataset.\n",
    "\n",
    "        This method loads the US county demographics data, applies any pre-processing transformations if specified,\n",
    "        and saves the processed data to the appropriate location.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        data = load_us_county_demos(self.raw_dir, self.name)\n",
    "\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}({len(self)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "\n",
    "def load_us_county_demos(path, dataset_name, year=2012):\n",
    "    edges_df = pd.read_csv(f\"{path}/county_graph.csv\")\n",
    "    stat = pd.read_csv(f\"{path}/county_stats_{year}.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "    keep_cols = [\n",
    "        \"FIPS\",\n",
    "        \"DEM\",\n",
    "        \"GOP\",\n",
    "        \"MedianIncome\",\n",
    "        \"MigraRate\",\n",
    "        \"BirthRate\",\n",
    "        \"DeathRate\",\n",
    "        \"BachelorRate\",\n",
    "        \"UnemploymentRate\",\n",
    "    ]\n",
    "    # Drop rows with missing values\n",
    "    stat = stat[keep_cols].dropna()\n",
    "\n",
    "    # Delete edges that are not present in stat df\n",
    "    unique_fips = stat[\"FIPS\"].unique()\n",
    "\n",
    "    src_ = edges_df[\"SRC\"].apply(lambda x: x in unique_fips)\n",
    "    dst_ = edges_df[\"DST\"].apply(lambda x: x in unique_fips)\n",
    "\n",
    "    edges_df = edges_df[src_ & dst_]\n",
    "\n",
    "    # Remove rows from stat df where edges_df['SRC'] or edges_df['DST'] are not present\n",
    "    stat = stat[stat[\"FIPS\"].isin(edges_df[\"SRC\"]) & stat[\"FIPS\"].isin(edges_df[\"DST\"])]\n",
    "    stat = stat.reset_index(drop=True)\n",
    "\n",
    "    # Remove rows where SRC == DST\n",
    "    edges_df = edges_df[edges_df[\"SRC\"] != edges_df[\"DST\"]]\n",
    "\n",
    "    # Get torch_geometric edge_index format\n",
    "    edge_index = torch.tensor(\n",
    "        np.stack([edges_df[\"SRC\"].to_numpy(), edges_df[\"DST\"].to_numpy()])\n",
    "    )\n",
    "\n",
    "    # Make edge_index undirected\n",
    "    edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
    "\n",
    "    # Convert edge_index back to pandas DataFrame\n",
    "    edges_df = pd.DataFrame(edge_index.numpy().T, columns=[\"SRC\", \"DST\"])\n",
    "\n",
    "    del edge_index\n",
    "\n",
    "    # Map stat['FIPS'].unique() to [0, ..., num_nodes]\n",
    "    fips_map = {fips: i for i, fips in enumerate(stat[\"FIPS\"].unique())}\n",
    "    stat[\"FIPS\"] = stat[\"FIPS\"].map(fips_map)\n",
    "\n",
    "    # Map edges_df['SRC'] and edges_df['DST'] to [0, ..., num_nodes]\n",
    "    edges_df[\"SRC\"] = edges_df[\"SRC\"].map(fips_map)\n",
    "    edges_df[\"DST\"] = edges_df[\"DST\"].map(fips_map)\n",
    "\n",
    "    # Get torch_geometric edge_index format\n",
    "    edge_index = torch.tensor(\n",
    "        np.stack([edges_df[\"SRC\"].to_numpy(), edges_df[\"DST\"].to_numpy()])\n",
    "    )\n",
    "\n",
    "    # Remove isolated nodes (Note: this function maps the nodes to [0, ..., num_nodes] automatically)\n",
    "    edge_index, _, mask = torch_geometric.utils.remove_isolated_nodes(edge_index)\n",
    "\n",
    "    # Conver mask to index\n",
    "    index = np.arange(mask.size(0))[mask]\n",
    "    stat = stat.iloc[index]\n",
    "    stat = stat.reset_index(drop=True)\n",
    "\n",
    "    # Get new values for FIPS from current index\n",
    "    # To understand why please print stat.iloc[[516, 517, 518, 519, 520]] for 2012 year\n",
    "    # Basically the FIPS values has been shifted\n",
    "    stat[\"FIPS\"] = stat.reset_index()[\"index\"]\n",
    "\n",
    "    # Create Election variable\n",
    "    stat[\"Election\"] = (stat[\"DEM\"] - stat[\"GOP\"]) / (stat[\"DEM\"] + stat[\"GOP\"])\n",
    "\n",
    "    # Drop DEM and GOP columns and FIPS\n",
    "    stat = stat.drop(columns=[\"DEM\", \"GOP\", \"FIPS\"])\n",
    "\n",
    "    # Prediction col\n",
    "    y_col = \"Election\"  # TODO: Define through config file\n",
    "    x_col = list(set(stat.columns).difference(set([y_col])))\n",
    "\n",
    "    stat[\"MedianIncome\"] = (\n",
    "        stat[\"MedianIncome\"]\n",
    "        .apply(lambda x: x.replace(\",\", \"\"))\n",
    "        .to_numpy()\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    x = stat[x_col].to_numpy()\n",
    "    y = stat[y_col].to_numpy()\n",
    "\n",
    "    data = torch_geometric.data.Data(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "a = CornelDataset(\n",
    "    root=\"/home/lev/projects/TopoBenchmarkX/datasets/graph\", name=\"US-county-demos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"data_seed\": 0,\n",
    "    \"data_split_dir\": \"/home/lev/projects/TopoBenchmarkX/datasets/data_splits/US-county-demos\",\n",
    "    \"train_prop\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3107, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m a\u001b[38;5;241m.\u001b[39mk\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'k'"
     ]
    }
   ],
   "source": [
    "a = {\"k\": 1}\n",
    "a.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.read_csv(\n",
    "    \"/home/lev/projects/TopoBenchmarkX/datasets/graph/US-county-demos-2012/raw/US-county-demos/county_stats_2016.csv\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FIPS', 'County', 'DEM', 'GOP', 'MedianIncome', 'MigraRate',\n",
       "       'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Election',\n",
       " 'MedianIncome',\n",
       " 'MigraRate',\n",
       " 'BirthRate',\n",
       " 'DeathRate',\n",
       " 'BachelorRate',\n",
       " 'UnemploymentRate')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    \"Election\",\n",
    "    \"MedianIncome\",\n",
    "    \"MigraRate\",\n",
    "    \"BirthRate\",\n",
    "    \"DeathRate\",\n",
    "    \"BachelorRate\",\n",
    "    \"UnemploymentRate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
